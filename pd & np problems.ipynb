{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c6be79",
   "metadata": {},
   "source": [
    "# Monthly Transactions II\n",
    " \n",
    "Table: Transactions\n",
    "\n",
    "    +----------------+---------+\n",
    "    | Column Name    | Type    |\n",
    "    +----------------+---------+\n",
    "    | id             | int     |\n",
    "    | country        | varchar |\n",
    "    | state          | enum    |\n",
    "    | amount         | int     |\n",
    "    | trans_date     | date    |\n",
    "    +----------------+---------+\n",
    "    id is the column of unique values of this table.\n",
    "    The table has information about incoming transactions.\n",
    "    The state column is an ENUM (category) of type [\"approved\", \"declined\"].\n",
    "Table: Chargebacks\n",
    "\n",
    "    +----------------+---------+\n",
    "    | Column Name    | Type    |\n",
    "    +----------------+---------+\n",
    "    | trans_id       | int     |\n",
    "    | trans_date     | date    |\n",
    "    +----------------+---------+\n",
    "    Chargebacks contains basic information regarding incoming chargebacks from some transactions placed in Transactions table.\n",
    "    trans_id is a foreign key (reference column) to the id column of Transactions table.\n",
    "    Each chargeback corresponds to a transaction made previously even if they were not approved.\n",
    "\n",
    "\n",
    "    Write a solution to find for each month and country: the number of approved transactions and their total amount, the number of chargebacks, and their total amount.\n",
    "\n",
    "    Note: In your solution, given the month and country, ignore rows with all zeros.\n",
    "\n",
    "    Return the result table in any order.\n",
    "\n",
    "    The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Transactions table:\n",
    "\n",
    "    +-----+---------+----------+--------+------------+\n",
    "    | id  | country | state    | amount | trans_date |\n",
    "    +-----+---------+----------+--------+------------+\n",
    "    | 101 | US      | approved | 1000   | 2019-05-18 |\n",
    "    | 102 | US      | declined | 2000   | 2019-05-19 |\n",
    "    | 103 | US      | approved | 3000   | 2019-06-10 |\n",
    "    | 104 | US      | declined | 4000   | 2019-06-13 |\n",
    "    | 105 | US      | approved | 5000   | 2019-06-15 |\n",
    "    +-----+---------+----------+--------+------------+\n",
    "Chargebacks table:\n",
    "\n",
    "    +----------+------------+\n",
    "    | trans_id | trans_date |\n",
    "    +----------+------------+\n",
    "    | 102      | 2019-05-29 |\n",
    "    | 101      | 2019-06-30 |\n",
    "    | 105      | 2019-09-18 |\n",
    "    +----------+------------+\n",
    "Output: \n",
    "\n",
    "    +---------+---------+----------------+-----------------+------------------+-------------------+\n",
    "    | month   | country | approved_count | approved_amount | chargeback_count | chargeback_amount |\n",
    "    +---------+---------+----------------+-----------------+------------------+-------------------+\n",
    "    | 2019-05 | US      | 1              | 1000            | 1                | 2000              |\n",
    "    | 2019-06 | US      | 2              | 8000            | 1                | 1000              |\n",
    "    | 2019-09 | US      | 0              | 0               | 1                | 5000              |\n",
    "    +---------+---------+----------------+-----------------+------------------+-------------------+\n",
    " \n",
    "Initial Ideas\n",
    "\n",
    "    The goal of the monthly_transactions function is to process two data tables: one for transactions and another for chargebacks. The function aims to summarize the number of approved transactions and chargebacks by month and country. The output should reflect counts and amounts for both approved transactions and chargebacks, allowing for an effective comparison.\n",
    "\n",
    "Steps\n",
    "\n",
    "    Data Preparation: Convert the transaction dates to a consistent format representing year and month (YYYY-MM).\n",
    "    Filtering Approved Transactions: Keep only the rows where transactions have an approved status.\n",
    "    Aggregation of Approved Transactions: Group by month and country, counting the number of approved transactions and summing their amounts.\n",
    "    Processing Chargebacks: Merge chargebacks with transactions to include relevant transaction details, and group by month and country to aggregate chargeback data.\n",
    "    Combining Results: Merge the summary of approved transactions with the chargeback summary, filling any missing values with zeros.\n",
    "    Formatting the Output: Ensure the output month format is correct and return the final DataFrame.    \n",
    "    \n",
    "Edge Cases\n",
    "\n",
    "    No Transactions: If the transactions DataFrame is empty, the output should only contain chargeback information if available.\n",
    "    No Chargebacks: If the chargebacks DataFrame is empty, the output should reflect only approved transactions with chargeback counts and amounts set to zero.\n",
    "    Same Month for Multiple Transactions: Ensure that the aggregation works correctly when multiple transactions or chargebacks occur in the same month.\n",
    "    \n",
    "Complexity Analysis\n",
    "\n",
    "    Time Complexity: The function primarily involves filtering and aggregating DataFrames, resulting in a complexity of O(n log n) due to the grouping and aggregation operations, where n is the number of transactions and chargebacks.\n",
    "    Space Complexity: The space complexity is O(m + k), where m is the number of unique months for transactions and chargebacks, and k is the number of countries.\n",
    "    \n",
    "Follow-Up Questions and Answers\n",
    "Q: What would happen if there are duplicate transactions?\n",
    "\n",
    "    A: The function assumes that transactions are unique by id. Duplicate entries could skew the results, so it’s advisable to handle duplicates before processing.\n",
    "Q: How would you modify the function to handle multiple countries?\n",
    "\n",
    "    A: The function already supports multiple countries through its grouping operations. Additional countries would be naturally included in the aggregation.\n",
    "Q: How can we extend this function to include a comparison with previous months?\n",
    "\n",
    "    A: We could add additional logic to calculate differences between months by storing the previous month’s totals in a separate DataFrame and then performing a join or calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35088fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def monthly_transactions(transactions: pd.DataFrame, chargebacks: pd.DataFrame) -> pd.DataFrame:\n",
    "    transactions['trans_date'] = pd.to_datetime(transactions['trans_date'])\n",
    "    chargebacks['trans_date'] = pd.to_datetime(chargebacks['trans_date'])\n",
    "\n",
    "    # Step 1: Format transaction dates to month format\n",
    "    transactions['trans_date'] = transactions['trans_date'].dt.strftime('%y-%m')\n",
    "    \n",
    "    # Step 2: Merge chargebacks with transactions\n",
    "    chargebacks = chargebacks.merge(transactions, left_on='trans_id', right_on='id', how='inner')[['trans_id','trans_date','country','amount']]\n",
    "\n",
    "    # Step 3: Filter approved transactions\n",
    "    transactions = transactions[transactions['state'] == 'approved']\n",
    "    \n",
    "    # Step 4: Group approved transactions\n",
    "    result = transactions.groupby(['trans_date', 'country']).agg(\n",
    "        approved_count=('state', 'count'),\n",
    "        approved_amount=('amount', 'sum')\n",
    "    ).reset_index().rename(columns={'trans_date': 'month'})\n",
    "\n",
    "    # Step 5: Process chargebacks\n",
    "    chargebacks['month'] = chargebacks['trans_date'].dt.strftime('%y-%m')\n",
    "    chargebacks = chargebacks.groupby(['month', 'country']).agg(\n",
    "        chargeback_count=('trans_id', 'count'),\n",
    "        chargeback_amount=('amount', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Step 6: Combine results\n",
    "    combine_df = result.merge(chargebacks, on=['month', 'country'], how='outer').fillna(0)\n",
    "    \n",
    "    # Formatting month\n",
    "    combine_df['month'] = '20' + combine_df['month']\n",
    "    \n",
    "    return combine_df\n",
    "\n",
    "# Example input data\n",
    "transactions_data = {\n",
    "    'id': [101, 102, 103, 104, 105],\n",
    "    'country': ['US', 'US', 'US', 'US', 'US'],\n",
    "    'state': ['approved', 'declined', 'approved', 'declined', 'approved'],\n",
    "    'amount': [1000, 2000, 3000, 4000, 5000],\n",
    "    'trans_date': ['2019-05-18', '2019-05-19', '2019-06-10', '2019-06-13', '2019-06-15']\n",
    "}\n",
    "\n",
    "chargebacks_data = {\n",
    "    'trans_id': [102, 101, 105],\n",
    "    'trans_date': ['2019-05-29', '2019-06-30', '2019-09-18']\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "transactions_df = pd.DataFrame(transactions_data)\n",
    "chargebacks_df = pd.DataFrame(chargebacks_data)\n",
    "\n",
    "# Get monthly transactions\n",
    "result_df = monthly_transactions(transactions_df, chargebacks_df)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07a493",
   "metadata": {},
   "source": [
    "# Game Play Analysis III\n",
    " \n",
    "Table: Activity\n",
    "\n",
    "    +--------------+---------+\n",
    "    | Column Name  | Type    |\n",
    "    +--------------+---------+\n",
    "    | player_id    | int     |\n",
    "    | device_id    | int     |\n",
    "    | event_date   | date    |\n",
    "    | games_played | int     |\n",
    "    +--------------+---------+\n",
    "    (player_id, event_date) is the primary key (column with unique values) of this table.\n",
    "    This table shows the activity of players of some games.\n",
    "    Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device.\n",
    "\n",
    "\n",
    "    Write a solution to report for each player and date, how many games played so far by the player. That is, the total number of games played by the player until that date. Check the example for clarity.\n",
    "\n",
    "    Return the result table in any order.\n",
    "\n",
    "    The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "\n",
    "    +-----------+-----------+------------+--------------+\n",
    "    | player_id | device_id | event_date | games_played |\n",
    "    +-----------+-----------+------------+--------------+\n",
    "    | 1         | 2         | 2016-03-01 | 5            |\n",
    "    | 1         | 2         | 2016-05-02 | 6            |\n",
    "    | 1         | 3         | 2017-06-25 | 1            |\n",
    "    | 3         | 1         | 2016-03-02 | 0            |\n",
    "    | 3         | 4         | 2018-07-03 | 5            |\n",
    "    +-----------+-----------+------------+--------------+\n",
    "Output: \n",
    "\n",
    "    +-----------+------------+---------------------+\n",
    "    | player_id | event_date | games_played_so_far |\n",
    "    +-----------+------------+---------------------+\n",
    "    | 1         | 2016-03-01 | 5                   |\n",
    "    | 1         | 2016-05-02 | 11                  |\n",
    "    | 1         | 2017-06-25 | 12                  |\n",
    "    | 3         | 2016-03-02 | 0                   |\n",
    "    | 3         | 2018-07-03 | 5                   |\n",
    "    +-----------+------------+---------------------+\n",
    "Explanation: \n",
    "\n",
    "    For the player with id 1, 5 + 6 = 11 games played by 2016-05-02, and 5 + 6 + 1 = 12 games played by 2017-06-25.\n",
    "    For the player with id 3, 0 + 5 = 5 games played by 2018-07-03.\n",
    "    Note that for each player we only care about the days when the player logged in.\n",
    "\n",
    "Explanation of the Code\n",
    "\n",
    "    Sorting: The first step sorts the DataFrame by player_id and event_date. This is essential because we need to calculate the cumulative sum of games played in chronological order for each player.\n",
    "\n",
    "    Cumulative Sum Calculation: We use the groupby method to group the data by player_id and then apply cumsum() on the games_played column. This calculates the cumulative number of games played by each player up to each event date.\n",
    "\n",
    "    Selecting Relevant Columns: After computing the cumulative sum, we create a new DataFrame called result that contains only the player_id, event_date, and the newly computed games_played_so_far.\n",
    "\n",
    "Edge Cases\n",
    "\n",
    "    No Activity: If the activity DataFrame is empty, the function will return an empty DataFrame.\n",
    "    Multiple Entries on the Same Day: If a player logs multiple entries on the same day, the cumulative sum will consider all entries for that day.\n",
    "    Different Players: The implementation inherently supports multiple players, ensuring that the cumulative sums are calculated independently for each player.\n",
    "    \n",
    "Complexity Analysis\n",
    "\n",
    "    Time Complexity: The sorting operation dominates the complexity, making it O(n log n), where n is the number of rows in the DataFrame. The grouping and cumulative sum operation is O(n).\n",
    "    Space Complexity: The space complexity is O(n) for storing the cumulative sums in the new column.\n",
    "\n",
    "Follow-Up Questions and Answers\n",
    "Q: How would the function handle players with no games played?\n",
    "\n",
    "    A: The function will correctly output zero for those players on their event dates.\n",
    "Q: Can this function be adapted for more detailed metrics?\n",
    "\n",
    "    A: Yes, it could be expanded to include metrics like average games played per session or total session time, requiring additional data.\n",
    "Q: What would happen if games_played contained negative values?\n",
    "\n",
    "    A: The cumulative sum would still calculate, but negative values could lead to incorrect totals. Data validation would be necessary to handle such cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f125ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   player_id event_date  games_played_so_far\n",
      "0          1 2016-03-01                    5\n",
      "1          1 2016-05-02                   11\n",
      "2          1 2017-06-25                   12\n",
      "3          3 2016-03-02                    0\n",
      "4          3 2018-07-03                    5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def gameplay_analysis(activity: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Step 1: Sort the DataFrame by player_id and event_date\n",
    "    activity = activity.sort_values(by=['player_id', 'event_date'])\n",
    "    \n",
    "    # Step 2: Group by player_id and calculate the cumulative sum of games_played\n",
    "    activity['games_played_so_far'] = activity.groupby('player_id')['games_played'].cumsum()\n",
    "    \n",
    "    # Step 3: Select the relevant columns for output\n",
    "    result = activity[['player_id', 'event_date', 'games_played_so_far']]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "activity_data = {\n",
    "    'player_id': [1, 1, 1, 3, 3],\n",
    "    'device_id': [2, 2, 3, 1, 4],\n",
    "    'event_date': ['2016-03-01', '2016-05-02', '2017-06-25', '2016-03-02', '2018-07-03'],\n",
    "    'games_played': [5, 6, 1, 0, 5]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "activity_df = pd.DataFrame(activity_data)\n",
    "activity_df['event_date'] = pd.to_datetime(activity_df['event_date'])\n",
    "\n",
    "# Get the result\n",
    "result_df = gameplay_analysis(activity_df)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68dc518",
   "metadata": {},
   "source": [
    "# Count Student Number in Departments\n",
    " \n",
    "Table: Student\n",
    "\n",
    "    +--------------+---------+\n",
    "    | Column Name  | Type    |\n",
    "    +--------------+---------+\n",
    "    | student_id   | int     |\n",
    "    | student_name | varchar |\n",
    "    | gender       | varchar |\n",
    "    | dept_id      | int     |\n",
    "    +--------------+---------+\n",
    "    student_id is the primary key (column with unique values) for this table.\n",
    "    dept_id is a foreign key (reference column) to dept_id in the Department tables.\n",
    "    Each row of this table indicates the name of a student, their gender, and the id of their department.\n",
    "\n",
    "\n",
    "Table: Department\n",
    "\n",
    "    +-------------+---------+\n",
    "    | Column Name | Type    |\n",
    "    +-------------+---------+\n",
    "    | dept_id     | int     |\n",
    "    | dept_name   | varchar |\n",
    "    +-------------+---------+\n",
    "    dept_id is the primary key (column with unique values) for this table.\n",
    "    Each row of this table contains the id and the name of a department.\n",
    "\n",
    "\n",
    "    Write a solution to report the respective department name and number of students majoring in each department for all departments in the Department table (even ones with no current students).\n",
    "\n",
    "    Return the result table ordered by student_number in descending order. In case of a tie, order them by dept_name alphabetically.\n",
    "\n",
    "    The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Student table:\n",
    "\n",
    "    +------------+--------------+--------+---------+\n",
    "    | student_id | student_name | gender | dept_id |\n",
    "    +------------+--------------+--------+---------+\n",
    "    | 1          | Jack         | M      | 1       |\n",
    "    | 2          | Jane         | F      | 1       |\n",
    "    | 3          | Mark         | M      | 2       |\n",
    "    +------------+--------------+--------+---------+\n",
    "Department table:\n",
    "\n",
    "    +---------+-------------+\n",
    "    | dept_id | dept_name   |\n",
    "    +---------+-------------+\n",
    "    | 1       | Engineering |\n",
    "    | 2       | Science     |\n",
    "    | 3       | Law         |\n",
    "    +---------+-------------+\n",
    "Output: \n",
    "\n",
    "    +-------------+----------------+\n",
    "    | dept_name   | student_number |\n",
    "    +-------------+----------------+\n",
    "    | Engineering | 2              |\n",
    "    | Science     | 1              |\n",
    "    | Law         | 0              |\n",
    "    +-------------+----------------+\n",
    "    \n",
    "Initial Ideas\n",
    "\n",
    "    The problem requires counting students in each department and ensuring that departments with no students are included in the output. This suggests a need for a left join between the Student and Department tables to retain all department records.\n",
    "\n",
    "Steps\n",
    "\n",
    "    Group and Count: First, group the Student DataFrame by dept_id to count the number of students in each department.\n",
    "    Merge: Perform a left join with the Department DataFrame to associate department names with their respective student counts.\n",
    "    Handle Missing Values: Replace NaN values in the student_number column with 0 to reflect departments with no students.\n",
    "    Sort: Order the results by student_number in descending order, and alphabetically by dept_name for ties.\n",
    "    Select Relevant Columns: Return only the dept_name and student_number columns.\n",
    "    \n",
    "Edge Cases\n",
    "\n",
    "    No Students: If the Student table is empty, all departments should return with student_number as 0.\n",
    "    No Departments: If the Department table is empty, the result should also be an empty DataFrame.\n",
    "    All Departments Have Students: The output should still correctly count and sort as specified.\n",
    "    Multiple Entries for the Same Student: Each student should be counted once; duplicates in the Student table should not inflate the counts.\n",
    "\n",
    "Complexity\n",
    "\n",
    "    Time Complexity: The overall complexity is O(n + m log m) where n is the number of students and m is the number of departments, mainly due to the sorting step.\n",
    "    Space Complexity: The space complexity is O(m + n) because we create additional DataFrames to hold counts and results.\n",
    "\n",
    "Follow-Up Questions and Answers\n",
    "\n",
    "Q: How would you modify this to include more details about students?\n",
    "\n",
    "    A: We could include additional columns from the Student DataFrame by modifying the merge and groupby operations to keep track of gender or other attributes.\n",
    "Q: What if we wanted to group students by gender within each department?\n",
    "\n",
    "    A: We would group the Student DataFrame by both dept_id and gender and then count students for each combination before merging with the Department table.\n",
    "Q: How would you handle cases where student data may be invalid (e.g., null values)?\n",
    "\n",
    "    A: We could add data cleaning steps before processing, such as dropping rows with null values in critical columns like dept_id.\n",
    "Q: Can you suggest ways to optimize performance for very large datasets?\n",
    "\n",
    "    A: Indexing the dept_id column in both DataFrames can improve join performance. Also, using more efficient data types or data storage formats (like Parquet) may help reduce memory usage and speed up operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f7e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dept_name  student_number\n",
      "0  Engineering               2\n",
      "1      Science               1\n",
      "2          Law               0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_students(student: pd.DataFrame, department: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Step 1: Group by department ID in the student DataFrame and count students\n",
    "    student_count = student.groupby('dept_id').size().reset_index(name='student_number')\n",
    "    \n",
    "    # Step 2: Merge the student count with the department DataFrame\n",
    "    result = pd.merge(department, student_count, how='left', left_on='dept_id', right_on='dept_id')\n",
    "    \n",
    "    # Step 3: Fill NaN values with 0 for departments without students\n",
    "    result['student_number'] = result['student_number'].fillna(0).astype(int)\n",
    "    \n",
    "    # Step 4: Sort the results by student_number descending, then by dept_name alphabetically\n",
    "    result = result.sort_values(by=['student_number', 'dept_name'], ascending=[False, True])\n",
    "    \n",
    "    # Step 5: Select relevant columns for output\n",
    "    return result[['dept_name', 'student_number']]\n",
    "\n",
    "# Sample Input Data\n",
    "student_data = {\n",
    "    'student_id': [1, 2, 3],\n",
    "    'student_name': ['Jack', 'Jane', 'Mark'],\n",
    "    'gender': ['M', 'F', 'M'],\n",
    "    'dept_id': [1, 1, 2]\n",
    "}\n",
    "\n",
    "department_data = {\n",
    "    'dept_id': [1, 2, 3],\n",
    "    'dept_name': ['Engineering', 'Science', 'Law']\n",
    "}\n",
    "\n",
    "# Creating DataFrames\n",
    "student_df = pd.DataFrame(student_data)\n",
    "department_df = pd.DataFrame(department_data)\n",
    "\n",
    "# Running the function\n",
    "output_df = count_students(student_df, department_df)\n",
    "\n",
    "# Displaying the output\n",
    "print(output_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1fc09",
   "metadata": {},
   "source": [
    "# Shortest Distance in a Plane\n",
    " \n",
    "Table: Point2D\n",
    "\n",
    "    +-------------+------+\n",
    "    | Column Name | Type |\n",
    "    +-------------+------+\n",
    "    | x           | int  |\n",
    "    | y           | int  |\n",
    "    +-------------+------+\n",
    "    (x, y) is the primary key column (combination of columns with unique values) for this table.\n",
    "    Each row of this table indicates the position of a point on the X-Y plane.\n",
    "\n",
    "\n",
    "    The distance between two points p1(x1, y1) and p2(x2, y2) is sqrt((x2 - x1)2 + (y2 - y1)2).\n",
    "\n",
    "    Write a solution to report the shortest distance between any two points from the Point2D table. Round the distance to two decimal points.\n",
    "\n",
    "    The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Point2D table:\n",
    "\n",
    "    +----+----+\n",
    "    | x  | y  |\n",
    "    +----+----+\n",
    "    | -1 | -1 |\n",
    "    | 0  | 0  |\n",
    "    | -1 | -2 |\n",
    "    +----+----+\n",
    "Output: \n",
    "\n",
    "    +----------+\n",
    "    | shortest |\n",
    "    +----------+\n",
    "    | 1.00     |\n",
    "    +----------+\n",
    "Explanation: The shortest distance is 1.00 from point (-1, -1) to (-1, 2).\n",
    "\n",
    "Problem Analysis\n",
    "\n",
    "    The task is to find the shortest Euclidean distance between any two points in a plane, given a table of (x, y) coordinates. We need to handle this in a way that avoids redundant calculations, leverages efficient operations, and provides accurate, rounded results.\n",
    "\n",
    "Initial Ideas\n",
    "\n",
    "    Self-Join Approach: We can join the table with itself to get all possible pairs of points and calculate the distance for each pair. Then, we'll select the minimum distance.\n",
    "    Avoiding Redundancy: Since distances are symmetric (distance from A to B is the same as from B to A), we only need unique pairs, not both directions.\n",
    "    Vectorized Calculations: Using Pandas and NumPy for efficient, vectorized operations will help keep calculations fast.\n",
    "\n",
    "Steps\n",
    "\n",
    "    Create All Pairs: Use a self-join on the table to create all possible pairs of points.\n",
    "    Filter Identical Pairs: Remove pairs where both points are the same, as the distance would be zero.\n",
    "    Compute Distance: For each pair of points, calculate the Euclidean distance.\n",
    "    Find Minimum Distance: Identify the shortest distance and round it to two decimal places.\n",
    "    \n",
    "Walkthrough Example\n",
    "\n",
    "Input:\n",
    "\n",
    "    Consider a table with the following points:.\n",
    "\n",
    "    | x  | y  |\n",
    "    |----|----|\n",
    "    | -1 | -1 |\n",
    "    |  0 |  0 |\n",
    "    | -1 | -2 |\n",
    "\n",
    "Execution:\n",
    "\n",
    "    Self-Join:\n",
    "        Creates all pairs:\n",
    "            | x_1 | y_1 | x_2 | y_2 |\n",
    "            |-----|-----|-----|-----|\n",
    "            | -1  | -1  | -1  | -1  |\n",
    "            | -1  | -1  |  0  |  0  |\n",
    "            | -1  | -1  | -1  | -2  |\n",
    "            |  0  |  0  | -1  | -1  |\n",
    "            |  0  |  0  |  0  |  0  |\n",
    "            |  0  |  0  | -1  | -2  |\n",
    "            | -1  | -2  | -1  | -1  |\n",
    "            | -1  | -2  |  0  |  0  |\n",
    "            | -1  | -2  | -1  | -2  |\n",
    "    \n",
    "    Filter Identical Pairs:\n",
    "        Remove pairs where (x_1, y_1) == (x_2, y_2).\n",
    "\n",
    "    Calculate Distance:\n",
    "\n",
    "        Apply the distance formula on remaining pairs, e.g., for (-1, -1) to (-1, -2), the distance is sqrt(((-1) - (-1))^2 + ((-2) - (-1))^2) = 1.0.\n",
    "    Find Minimum: Identify the minimum value among calculated distances, which is 1.0.\n",
    "\n",
    "Output:\n",
    "\n",
    "    +----------+\n",
    "    | shortest |\n",
    "    +----------+\n",
    "    | 1.00     |\n",
    "    +----------+\n",
    "Edge Cases\n",
    "\n",
    "    Single Point: If there’s only one point, there’s no pair to calculate, so we should handle this case by returning an empty or null result.\n",
    "    Identical Points: If all points are identical, the shortest distance would ideally be infinity or some indication of no valid pair.\n",
    "    Multiple Closest Pairs: If there are multiple pairs with the same minimum distance, the function should still return that minimum without considering pair count.\n",
    "    \n",
    "Complexity Analysis\n",
    "\n",
    "    Time Complexity: \n",
    "        O(n ^2), where n is the number of points, due to the need to examine all pairs.\n",
    "    Space Complexity: \n",
    "        O(n^2 ) as well, for storing all pairs of points.\n",
    "        \n",
    "Follow-up Questions and Answers\n",
    "What if we have a large dataset of points?\n",
    "\n",
    "    Consider using KD-trees or spatial indexing structures, which can reduce the complexity of nearest-neighbor search.\n",
    "How can this be optimized further?\n",
    "\n",
    "    Precompute unique pairs only without performing a full Cartesian product, perhaps by using combinations from itertools or applying half-matrix traversal.\n",
    "Would rounding earlier affect the accuracy?\n",
    "\n",
    "    Rounding should only happen on the final answer to avoid accumulating rounding errors during intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c3b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def shortest_distance(point2_d: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Perform a self-join with different suffixes to get all pairs\n",
    "    pairs = point2_d.merge(point2_d, how=\"cross\", suffixes=('_1', '_2'))\n",
    "    \n",
    "    # Filter out pairs where the points are the same\n",
    "    pairs = pairs[(pairs['x_1'] != pairs['x_2']) | (pairs['y_1'] != pairs['y_2'])]\n",
    "    \n",
    "    # Calculate Euclidean distance for each pair\n",
    "    pairs['distance'] = np.sqrt((pairs['x_2'] - pairs['x_1'])**2 + (pairs['y_2'] - pairs['y_1'])**2)\n",
    "    \n",
    "    # Find the minimum distance and round to two decimal points\n",
    "    shortest_distance = pairs['distance'].min().round(2)\n",
    "    \n",
    "    # Return result as a DataFrame with the specified format\n",
    "    return pd.DataFrame({'shortest': [shortest_distance]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4598d3",
   "metadata": {},
   "source": [
    "# Unpopular Books\n",
    " \n",
    "Table: Books\n",
    "\n",
    "    +----------------+---------+\n",
    "    | Column Name    | Type    |\n",
    "    +----------------+---------+\n",
    "    | book_id        | int     |\n",
    "    | name           | varchar |\n",
    "    | available_from | date    |\n",
    "    +----------------+---------+\n",
    "    book_id is the primary key (column with unique values) of this table.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "    +----------------+---------+\n",
    "    | Column Name    | Type    |\n",
    "    +----------------+---------+\n",
    "    | order_id       | int     |\n",
    "    | book_id        | int     |\n",
    "    | quantity       | int     |\n",
    "    | dispatch_date  | date    |\n",
    "    +----------------+---------+\n",
    "    order_id is the primary key (column with unique values) of this table.\n",
    "    book_id is a foreign key (reference column) to the Books table.\n",
    "\n",
    "\n",
    "    Write a solution to report the books that have sold less than 10 copies in the last year, excluding books that have been available for less than one month from today. Assume today is 2019-06-23.\n",
    "\n",
    "    Return the result table in any order.\n",
    "\n",
    "    The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Books table:\n",
    "\n",
    "    +---------+--------------------+----------------+\n",
    "    | book_id | name               | available_from |\n",
    "    +---------+--------------------+----------------+\n",
    "    | 1       | \"Kalila And Demna\" | 2010-01-01     |\n",
    "    | 2       | \"28 Letters\"       | 2012-05-12     |\n",
    "    | 3       | \"The Hobbit\"       | 2019-06-10     |\n",
    "    | 4       | \"13 Reasons Why\"   | 2019-06-01     |\n",
    "    | 5       | \"The Hunger Games\" | 2008-09-21     |\n",
    "    +---------+--------------------+----------------+\n",
    "Orders table:\n",
    "\n",
    "    +----------+---------+----------+---------------+\n",
    "    | order_id | book_id | quantity | dispatch_date |\n",
    "    +----------+---------+----------+---------------+\n",
    "    | 1        | 1       | 2        | 2018-07-26    |\n",
    "    | 2        | 1       | 1        | 2018-11-05    |\n",
    "    | 3        | 3       | 8        | 2019-06-11    |\n",
    "    | 4        | 4       | 6        | 2019-06-05    |\n",
    "    | 5        | 4       | 5        | 2019-06-20    |\n",
    "    | 6        | 5       | 9        | 2009-02-02    |\n",
    "    | 7        | 5       | 8        | 2010-04-13    |\n",
    "    +----------+---------+----------+---------------+\n",
    "Output: \n",
    "\n",
    "    +-----------+--------------------+\n",
    "    | book_id   | name               |\n",
    "    +-----------+--------------------+\n",
    "    | 1         | \"Kalila And Demna\" |\n",
    "    | 2         | \"28 Letters\"       |\n",
    "    | 5         | \"The Hunger Games\" |\n",
    "    +-----------+--------------------+\n",
    "    \n",
    "Step-by-Step Explanation\n",
    "\n",
    "Step 1 - Filtering Available Books:\n",
    "\n",
    "    The line books = books.loc[books.available_from + pd.DateOffset(30) < \"2019-06-23\"] filters the books DataFrame to include only those books that have been available for at least one month before the given date (2019-06-23).\n",
    "    It checks whether the available_from date plus 30 days is less than the specified date.\n",
    "Step 2 - Aggregating Order Quantities:\n",
    "\n",
    "    The line orders = orders.loc[orders.dispatch_date + pd.DateOffset(365) > \"2019-06-23\"] filters the orders DataFrame to include only those orders that were dispatched within the last year (365 days) from the given date.\n",
    "    Then, it groups the remaining orders by book_id and sums the quantity sold for each book with groupby(\"book_id\")[\"quantity\"].sum(). This results in a Series where the index is book_id and the values are the total quantities sold.\n",
    "Step 3 - Merging and Filtering:\n",
    "\n",
    "    The line books.merge(orders, on=\"book_id\", how=\"left\") merges the filtered books DataFrame with the aggregated orders DataFrame based on the book_id.\n",
    "    The how=\"left\" parameter ensures that all books are retained, even those without any sales (these will have NaN for the quantity).\n",
    "    fillna(0) replaces NaN values in the resulting DataFrame with 0, indicating that those books sold no copies.\n",
    "    Finally, .query(\"quantity < 10\") filters the merged DataFrame to keep only those books where the total quantity sold is less than 10.\n",
    "    The final output only selects the book_id and name columns using [['book_id', 'name']].\n",
    "\n",
    "Complexity Analysis\n",
    "\n",
    "Time Complexity:\n",
    "\n",
    "    Step 1: The filtering of books takes O(m), where m is the number of rows in the books DataFrame.\n",
    "    Step 2: The filtering of orders also takes  O(n), where n is the number of rows in the orders DataFrame. The grouping and summing operation takes O(n) as well.\n",
    "    Step 3: The merging operation is typically  O(m+k), where k is the number of rows in the orders after aggregation. The query operation is also O(m+k) in the worst case.\n",
    "    Overall Time Complexity: Therefore, the overall time complexity can be approximated as  O(m+n), where m is the number of books and  n is the number of orders.\n",
    "\n",
    "Space Complexity:\n",
    "\n",
    "    The space complexity mainly depends on the storage of the filtered DataFrames and the merged result. Thus, it can be considered O(m+k), where k is the number of unique book_ids in the orders DataFrame after grouping. This is a constant space requirement.\n",
    "    \n",
    "Edge Cases to Consider\n",
    "\n",
    "    No Sales: Books that have never been sold will still be included in the result if they meet the availability condition.\n",
    "    All Sold Books: If all books have sold 10 or more copies, the function should return an empty DataFrame.\n",
    "    Books Available Less Than a Month: If all books have been available for less than a month, the function should return an empty DataFrame.\n",
    "    Missing Values: If there are NaN values in the available_from or dispatch_date, this could affect filtering and should be handled or considered in pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef841ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpopular_books(books: pd.DataFrame, orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Step 1: Filter books available for at least one month before the given date\n",
    "    books = books.loc[books.available_from + pd.DateOffset(30) < \"2019-06-23\"]\n",
    "    \n",
    "    # Step 2: Aggregate orders to get the total quantity sold per book\n",
    "    orders = orders.loc[orders.dispatch_date + pd.DateOffset(365) > \"2019-06-23\"]\\\n",
    "                   .groupby(\"book_id\")[\"quantity\"].sum()\n",
    "    \n",
    "    # Step 3: Merge books with order quantities, filling NaN with 0\n",
    "    return books.merge(orders, on=\"book_id\", how=\"left\")\\\n",
    "                 .fillna(0)\\\n",
    "                 .query(\"quantity < 10\")[[\"book_id\", \"name\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b4982",
   "metadata": {},
   "source": [
    "# Account Balance\n",
    " \n",
    "Table: Transactions\n",
    "\n",
    "    +-------------+------+\n",
    "    | Column Name | Type |\n",
    "    +-------------+------+\n",
    "    | account_id  | int  |\n",
    "    | day         | date |\n",
    "    | type        | ENUM |\n",
    "    | amount      | int  |\n",
    "    +-------------+------+\n",
    "    (account_id, day) is the primary key (combination of columns with unique values) for this table.\n",
    "    Each row contains information about one transaction, including the transaction type, the day it occurred on, and the amount.\n",
    "    type is an ENUM (category) of the type ('Deposit','Withdraw') \n",
    "\n",
    "\n",
    "    Write a solution to report the balance of each user after each transaction. You may assume that the balance of each account before any transaction is 0 and that the balance will never be below 0 at any moment.\n",
    "\n",
    "    Return the result table in ascending order by account_id, then by day in case of a tie.\n",
    "\n",
    "    The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Transactions table:\n",
    "\n",
    "    +------------+------------+----------+--------+\n",
    "    | account_id | day        | type     | amount |\n",
    "    +------------+------------+----------+--------+\n",
    "    | 1          | 2021-11-07 | Deposit  | 2000   |\n",
    "    | 1          | 2021-11-09 | Withdraw | 1000   |\n",
    "    | 1          | 2021-11-11 | Deposit  | 3000   |\n",
    "    | 2          | 2021-12-07 | Deposit  | 7000   |\n",
    "    | 2          | 2021-12-12 | Withdraw | 7000   |\n",
    "    +------------+------------+----------+--------+\n",
    "Output: \n",
    "\n",
    "    +------------+------------+---------+\n",
    "    | account_id | day        | balance |\n",
    "    +------------+------------+---------+\n",
    "    | 1          | 2021-11-07 | 2000    |\n",
    "    | 1          | 2021-11-09 | 1000    |\n",
    "    | 1          | 2021-11-11 | 4000    |\n",
    "    | 2          | 2021-12-07 | 7000    |\n",
    "    | 2          | 2021-12-12 | 0       |\n",
    "    +------------+------------+---------+\n",
    "Explanation: \n",
    "\n",
    "    Account 1:\n",
    "    - Initial balance is 0.\n",
    "    - 2021-11-07 --> deposit 2000. Balance is 0 + 2000 = 2000.\n",
    "    - 2021-11-09 --> withdraw 1000. Balance is 2000 - 1000 = 1000.\n",
    "    - 2021-11-11 --> deposit 3000. Balance is 1000 + 3000 = 4000.\n",
    "    Account 2:\n",
    "    - Initial balance is 0.\n",
    "    - 2021-12-07 --> deposit 7000. Balance is 0 + 7000 = 7000.\n",
    "    - 2021-12-12 --> withdraw 7000. Balance is 7000 - 7000 = 0.\n",
    "    \n",
    "Initial Thoughts\n",
    "\n",
    "    The problem requires calculating the balance for each account after each transaction, starting from an initial balance of 0. The transactions can either be deposits or withdrawals, and we need to ensure that the balance does not drop below 0 at any point. The results should be organized by account_id and day in ascending order.\n",
    "\n",
    "Explanation of the Steps\n",
    "\n",
    "Initialization:\n",
    "\n",
    "    The function takes a DataFrame transactions as input, which contains the transaction details (account ID, day, type, and amount).\n",
    "Calculating Initial Balances:\n",
    "\n",
    "    A new column balance is created using np.where().\n",
    "    If the type is \"Deposit\", the amount is added to the balance; if it’s \"Withdraw\", the amount is negated (multiplied by -1).\n",
    "    This results in a balance column that reflects the net effect of each transaction.\n",
    "Sorting Transactions:\n",
    "\n",
    "    The DataFrame is sorted by account_id and day to ensure that transactions are processed chronologically for each account.\n",
    "Cumulative Sum:\n",
    "\n",
    "    The cumulative balance for each account is calculated using groupby() and cumsum(), which provides the running total of the balance for each account_id.\n",
    "Final Output:\n",
    "\n",
    "    The function returns a DataFrame containing account_id, day, and the computed balance, sorted by account_id and day.\n",
    "    \n",
    "Code Explanation\n",
    "\n",
    "    Importing Libraries: The code imports pandas for data manipulation and numpy for efficient numerical operations.\n",
    "\n",
    "    Creating Balance Column: The np.where() function creates the initial balance column, converting withdrawals to negative values. This simplifies balance calculations in the next steps.\n",
    "\n",
    "    Sorting: The sort_values() method arranges the transactions chronologically, which is crucial for accurate cumulative balance calculations.\n",
    "\n",
    "    Cumulative Sum: The groupby() and cumsum() methods calculate the running total for each account, effectively simulating the balance after each transaction.\n",
    "\n",
    "    Final DataFrame: The output is limited to the relevant columns and sorted to match the required format.\n",
    "\n",
    "Edge Cases\n",
    "\n",
    "    No Transactions: If the input DataFrame is empty, the output will also be an empty DataFrame.\n",
    "    Only Withdrawals: If an account has only withdrawals, the function will still calculate balances correctly, starting from 0.\n",
    "    Multiple Accounts with No Activity: If transactions are present for only some accounts, the function will accurately reflect the balances for those accounts without errors.\n",
    "    Transactions on the Same Day: The current implementation handles multiple transactions on the same day by calculating cumulative balances, ensuring correctness.\n",
    "    \n",
    "Complexity Analysis\n",
    "\n",
    "    Time Complexity: The time complexity is O(nlogn) primarily due to the sorting step, where n is the number of transactions.\n",
    "    Space Complexity: The space complexity is  O(n) due to the storage of intermediate and final DataFrame results.\n",
    "\n",
    "Follow-up Questions and Answers\n",
    "\n",
    "Q: How would you modify the function if the balance could go negative?\n",
    "\n",
    "    A: You would remove the check for withdrawals being negative and simply add the amounts without ensuring the balance stays above 0.\n",
    "Q: What if the amount is negative?\n",
    "\n",
    "    A: The current implementation assumes valid input, where amounts are non-negative for deposits and withdrawals; validation checks could be added if needed.\n",
    "Q: Can you optimize this function further?\n",
    "\n",
    "    A: The current approach is efficient, but if many transactions are involved, preprocessing or summarizing data before this step could enhance performance.\n",
    "Q: How do you handle simultaneous transactions on the same day?\n",
    "\n",
    "    A: The function processes transactions in the order they are provided, so any simultaneous transactions are handled based on their input order, ensuring the correct cumulative balances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f15b9",
   "metadata": {},
   "source": [
    "# Queries Quality and Percentage\n",
    " \n",
    "Table: Queries\n",
    "\n",
    "    +-------------+---------+\n",
    "    | Column Name | Type    |\n",
    "    +-------------+---------+\n",
    "    | query_name  | varchar |\n",
    "    | result      | varchar |\n",
    "    | position    | int     |\n",
    "    | rating      | int     |\n",
    "    +-------------+---------+\n",
    "    This table may have duplicate rows.\n",
    "    This table contains information collected from some queries on a database.\n",
    "    The position column has a value from 1 to 500.\n",
    "    The rating column has a value from 1 to 5. Query with rating less than 3 is a poor query.\n",
    "\n",
    "\n",
    "    We define query quality as:\n",
    "\n",
    "    The average of the ratio between query rating and its position.\n",
    "\n",
    "    We also define poor query percentage as:\n",
    "\n",
    "    The percentage of all queries with rating less than 3.\n",
    "\n",
    "    Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "\n",
    "    Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "    Return the result table in any order.\n",
    "\n",
    "    The result format is in the following example.\n",
    "\n",
    "\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Queries table:\n",
    "\n",
    "    +------------+-------------------+----------+--------+\n",
    "    | query_name | result            | position | rating |\n",
    "    +------------+-------------------+----------+--------+\n",
    "    | Dog        | Golden Retriever  | 1        | 5      |\n",
    "    | Dog        | German Shepherd   | 2        | 5      |\n",
    "    | Dog        | Mule              | 200      | 1      |\n",
    "    | Cat        | Shirazi           | 5        | 2      |\n",
    "    | Cat        | Siamese           | 3        | 3      |\n",
    "    | Cat        | Sphynx            | 7        | 4      |\n",
    "    +------------+-------------------+----------+--------+\n",
    "Output: \n",
    "\n",
    "    +------------+---------+-----------------------+\n",
    "    | query_name | quality | poor_query_percentage |\n",
    "    +------------+---------+-----------------------+\n",
    "    | Dog        | 2.50    | 33.33                 |\n",
    "    | Cat        | 0.66    | 33.33                 |\n",
    "    +------------+---------+-----------------------+\n",
    "Explanation: \n",
    "\n",
    "    Dog queries quality is ((5 / 1) + (5 / 2) + (1 / 200)) / 3 = 2.50\n",
    "    Dog queries poor_ query_percentage is (1 / 3) * 100 = 33.33\n",
    "\n",
    "    Cat queries quality equals ((2 / 5) + (3 / 3) + (4 / 7)) / 3 = 0.66\n",
    "    Cat queries poor_ query_percentage is (1 / 3) * 100 = 33.33\n",
    "    \n",
    "Explanation of the Code\n",
    "\n",
    "    Calculate Quality Ratio:\n",
    "\n",
    "        queries['quality'] creates a column with the rating / position ratio for each query, defining the quality score for each entry.\n",
    "    Calculate Poor Query Percentage:\n",
    "\n",
    "        queries['poor_query_percentage'] assigns a value of 100 to entries where rating < 3 (indicating poor queries). This results in 100 for poor queries and 0 otherwise.\n",
    "    Group by Query Name:\n",
    "\n",
    "        The groupby('query_name')[['quality', 'poor_query_percentage']].mean() calculates the mean of quality and poor_query_percentage for each query_name. The apply(lambda x: round(x + 1e-9, 2)) ensures the values are rounded to two decimal places, addressing any floating-point imprecision.\n",
    "    Return the Results:\n",
    "\n",
    "        reset_index() converts the grouped results back to a DataFrame format with query_name as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47522cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  query_name  quality  poor_query_percentage\n",
      "0        Cat     0.66                  33.33\n",
      "1        Dog     2.50                  33.33\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def queries_stats(queries: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Step 1: Calculate the quality ratio for each query\n",
    "    queries['quality'] = queries.rating / queries.position\n",
    "\n",
    "    # Step 2: Calculate poor query percentage\n",
    "    queries['poor_query_percentage'] = (queries.rating < 3) * 100\n",
    "\n",
    "    # Step 3: Group by query_name and compute the mean for quality and poor query percentage\n",
    "    # Then, round the results to 2 decimal places\n",
    "    return queries.groupby('query_name')[['quality', 'poor_query_percentage']]\\\n",
    "                  .mean().apply(lambda x: round(x + 1e-9, 2)).reset_index()\n",
    "\n",
    "data = {\n",
    "    \"query_name\": [\"Dog\", \"Dog\", \"Dog\", \"Cat\", \"Cat\", \"Cat\"],\n",
    "    \"result\": [\"Golden Retriever\", \"German Shepherd\", \"Mule\", \"Shirazi\", \"Siamese\", \"Sphynx\"],\n",
    "    \"position\": [1, 2, 200, 5, 3, 7],\n",
    "    \"rating\": [5, 5, 1, 2, 3, 4]\n",
    "}\n",
    "queries = pd.DataFrame(data)\n",
    "print(queries_stats(queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61dbb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
